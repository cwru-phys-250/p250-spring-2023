{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Numerical Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The main focus of this lab is to study numerical derivatives using center differencing and Richardson extrapolation.  We will also learn about the dangers of calculating numerical derivatives of noisy data.  Do *not* use any loops in this lab! (Besides the ones that appear in the `richardson_center` function, of course.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "As always, initialize your environment now by loading all modules required and setting up the plotting environment.\n",
    "\n",
    "We will be using random numbers in this lab so create a random number generator in this initialization cell using\n",
    "```\n",
    "rng = np.random.default_rng()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8be18dc62a1fe5433231605117859310",
     "grade": true,
     "grade_id": "cell-f18e76668ed9fa31",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Data\n",
    "\n",
    "One reason why calculating numerical derivatives is not too common is that calculating numerical derivatives of data is something that must be done with great care. The main reason for this is that data is noisy. Here you will explore that. We again consider a simple function,\n",
    "$$ f(\\theta) = \\sin\\theta, $$\n",
    "for which the derivative is simply\n",
    "$$ f'(\\theta) = \\cos\\theta. $$\n",
    "\n",
    "Suppose we take measurements of this function at 20 equally spaced points in the interval $0\\le\\theta\\le 2\\pi$.  An actual measurement has uncertainties which are often Gaussian distributed. We will model our errors by *adding* Gaussian random noise to the true values from a distribution with variance (width) of 0.2. Recall from Lab 0 that we can choose Gaussian random variables using the `normal` function as `rng.normal(size=N)` to generate `N` values. This will return random values with variance 1, to change this to 0.2 just multiply the returned values by 0.2 before adding it to the true function values.\n",
    "\n",
    "Once you have generated the \"data\" we want to take its derivative using center differencing.  Since center differencing uses values to the left and right of where we are calculating the derivative we are unable to calculate it at the first and last points. (We could correct this by using forward and backward differencing at these points or the higher order methods discussed in class, but will not do so here.) How do we calculate all the required derivatives without writing a loop? Once again we will use array slicing! (At this point you should start to appreciate that array slicing is very powerful. We will see it in more detail soon.) Notice that for $N$ points we want to calculate the derivatives\n",
    "\\begin{align}\n",
    " f'_1 &= \\frac{f_2-f_0}{2h}, \\\\\n",
    " f'_2 &= \\frac{f_3-f_1}{2h}, \\\\\n",
    " \\vdots &= \\vdots \\\\\n",
    " f'_{n-2} &= \\frac{f_{n-1}-f_{n-3}}{2h}.\n",
    " \\end{align}\n",
    "To calculate all the derivatives we are taking the values $f_2$, $f_3$, $\\ldots$, $f_{n-1}$ and subtracting off $f_0$, $f_1$, $\\ldots$, $f_{n-3}$. Given an array, `a`, we can access the first set of values as `a[2:]` and the second set as `a[:-2]`. (Read that again carefully!) Using this slicing in the calculation we will get an array back with $n-2$ entries corresponding to derivatives at the points $\\theta_1$, $\\theta_2$, $\\ldots$, $\\theta_{n-2}$. Given an array of values `theta` as specified above, how can we slice it to only get these values? (This will be useful for the plot below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all this together and produce a single plot.\n",
    "1. Create your noisy data.\n",
    "2. Plot points for the noisy data and a (smooth) curve showing the true function from the which the data was generated.\n",
    "3. Calculate the numerical derivative of your noisy data using center differencing.\n",
    "4. Plot points for the derivative of the noisy data and a (smooth) curve showing the true derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f077f945661608ae5ddb85a3827ed90f",
     "grade": true,
     "grade_id": "cell-14ebc68ecfdef6ec",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your results briefly describe the danger of taking derivative of noisy data. (You may want to run you code a few times to see how things change when different random data is generated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab90e4ae2b18511a757ab933ea9c19cc",
     "grade": true,
     "grade_id": "cell-ffdeca92225d01ed",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Bessel Functions\n",
    "\n",
    "The solution to many physical problems involve special functions.  One example is the set of Bessel functions. If you have not encountered them yet, you will.They typically show up in systems with cylindrical symmetry. There are many types of Bessel functions but we will focus on the ordinary Bessel functions of order $\\nu$ and of the first kind, $J_\\nu(x)$, and the second kind, $Y_\\nu(x)$ (sometimes also called the Neumann function and denoted $N_\\nu(x)$). Note that we are considering $\\nu$ to be a real number.\n",
    "\n",
    "Here we will consider taking the derivative with respect to the order, $\\nu$, **not the derivative with respect to $x$**. Admittedly this is rather a contrived example, but it is difficult to find an interesting and compelling example of numerical derivatives.\n",
    "\n",
    "To begin, we will take the derivative evaluated at $\\nu=0$. Though this derivative seems strange, it can be done analytically to get the known result,\n",
    "$$ \\left. \\frac{\\mathrm{d}J_\\nu(x)}{\\mathrm{d}\\nu} \\right|_{\\nu=0} = \\frac\\pi{2} Y_0(x). $$\n",
    "We will compare this result to numerical calculations using forward and center differencing.  **Note:** We are calculating the *derivative with respect to the order*, $\\nu$, **not** with respect to the argument $x$. We do not normally think about taking derivatives like this, but we certainly can and will do so here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Functions in Python\n",
    "\n",
    "The Bessel functions and many of the other important special functions are not part of the `numpy` module. The simplest way to access such functions for us will be with the module `scipy.special`. For a list of the special functions (and how to use them) check the documentation!\n",
    "\n",
    "Suppose we `import scipy.special as sf`. For us the relevant functions are `sf.jv` or `sf.jn` for Bessel functions of the first kind and `sf.yv` or `sf.yn` for the second kind. Here the `v` in the name represents what we have been calling $\\nu$ and is used when the order is a real number.  Similarly, `n` in the name represents when $\\nu=n$ is an integer. Furthermore, for small (integer) values of $n$ there is often an explicit implementation of that function. For example the special function $Y_0(x)$ can be called as `sf.y0(x)`, as `sf.yn(0,x)`, or as `sf.yv(0,x)`.  You should prefer the first over the second and the second over the third. In other words, it is typically best to use the most specific form of the function that is available. Again, check the documentation just like we do when going over examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Calculation\n",
    "\n",
    "Calculate the derivative for $x=2.5$ of $J_\\nu(x)$ at $\\nu=0$. Let $h=0.5$. Evaluate this derivative using both forward and center differencing for step sizes of both $h$ and $h/2$. Print the fractional error for all cases. **Note:** Recall that we are taking the derivative with respect to $\\nu$ **not** with respect to $x$. Also recall the lesson we learned when talking about limited precision. What is the \"right\" way to calculate the step size, $h$? Does it matter here? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b56df89560b965a8e04db60b3519cfe8",
     "grade": true,
     "grade_id": "cell-b5b8252bc3e74e96",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Richardson Extrapolation\n",
    "\n",
    "Next we want to study the same case but now with Richardson extrapolation applied to center differencing. Provide the code for performing Richardson extrapolation below. Yes, you *can* just copy this from the example or the prelab!\n",
    "\n",
    "*Code for Richardson Extrapolation of Center Differencing:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3f813fcffd98bdce098b299f0afb074",
     "grade": true,
     "grade_id": "cell-bc73f4248b7d17d2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate 15 steps of Richardson extrapolation for the case we have been studying. Produce a figure showing the absolute error of the best estimate from Richardson extrapolation at each step. Also include the convergence rate in this plot. Notice that we are taking the derivative using the function `sf.jv` and this function takes two arguments. How do we handle that? (*Hint*: We **do not** define a new function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a183e8164944e00a374e5f531b1899e",
     "grade": true,
     "grade_id": "cell-1fec1112c378e76b",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the error and convergence rate agree fairly and are decreasing until about step 6. What happens here? Why does the error flatten out and then start to grow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19bfbefeaf282970d7e56126952f9534",
     "grade": true,
     "grade_id": "cell-a13a4f6038f14017",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative at Arbitrary Integer Order\n",
    "\n",
    "The derivative can actually be calculated at any arbitrary integer order, $n$. Mathematically this is given by\n",
    "$$ \\left. \\frac{\\mathrm{d}J_\\nu(x)}{\\mathrm{d}\\nu} \\right|_{\\nu=n} = \\frac\\pi{2} Y_n(x) + \\frac{n!}{2(x/2)^n} \\sum_{k=0}^{n-1} \\frac{(x/2)^k J_k(x)}{(n-k) k!}. $$\n",
    "Here we will explore this more general relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Order\n",
    "\n",
    "Calculate the derivative for $n=4$ using Richardson extrapolation with 8 steps. Print the *fractional error* in the best estimate. Recall from the prelab that we can calculate the true value without using loops by instead using the `sum` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e41a24e77fb7bae7474b677d69339cf",
     "grade": true,
     "grade_id": "cell-eea56648919f368a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Order\n",
    "\n",
    "You may wonder why we are bothering with the numerical derivative since we have an analytic expression. It is not *just* to keep you busy. There are occasions when the analytic expression is not numerically useful  This is one such case.\n",
    "\n",
    "Calculate the derivative again using 8 steps of Richardson extrapolation and the analytic expression but now for $\\nu=40$. Print the best estimate from Richardson extrapolation and also the value from the analytic expression. (*Note:* It will be convenient for the discussion below to calculate and store the two terms in the analytic expression separately.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "555534b586b53903bb89349f9e4aed56",
     "grade": true,
     "grade_id": "cell-8b90cb35f6d65b35",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers should be entirely inconsistent. What happened and which one do you believe? To explore this print the two terms from the analytic expression. Do you see a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9abc533adae4b0d0a38852a9d30afe68",
     "grade": true,
     "grade_id": "cell-79cfdea6a5f262af",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous part, which estimate do you believe is the correct one for the derivative? Explain why you believe this estimate and what went wrong with the other one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a1b6ff97e6793e7f8a68ed89994f57c",
     "grade": true,
     "grade_id": "cell-9ef68cce85d77ef0",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Member Participation\n",
    "\n",
    "See Lab00 for instructions on turning in labs. We will follow this procedure the entire semester.\n",
    "\n",
    "In the following cell enter the *Case ID* for each student in the group who participated in this lab. Again, see Lab00 for more details. It is expected that you have read and understood those details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "authors": [
   {
    "name": "Craig J Copi",
    "semester": "Spring 2019"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
