{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numbers and Precision\n",
    "\n",
    "A brief exploration of why it is so hard to get computers to do what we want, or, why computational physics **is** a separate field of study (to go along with experimental and theoretical physics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Arithmetic\n",
    "\n",
    "An advantage of a scripting language and of a nice interface like the Jupyter notebook is that we can easily test things without needed to write lines and lines of boiler plate code.  Here we will test some simple arithmetic.\n",
    "\n",
    "We begin by adding integers and testing for the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 2 == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result is `True`, so it is in fact true that 1+2 is equal to 3!\n",
    "\n",
    "We now test something very similar: just divide the expression by 10, what could change?\n",
    "\n",
    "(*Note:* **Never do this is real code!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 + 0.2 == 0.3 # WRONG CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it claims this result is `False`!  How can the computer be so wrong!  It must be broken!  The sky is falling! *Etc.*\n",
    "\n",
    "What we are encountering here is the heart of the difficulty and annoyance of using computers for numerical work.  How can it possibly get this wrong?\n",
    "\n",
    "#### Finite Precision\n",
    "\n",
    "Let's see what it gives for the value of the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30000000000000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 + 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it does **not** give 0.3, there is a small \"error\" in the calculation.\n",
    "\n",
    "If you are aware of this issue then you might know that computers cannot represent every number exactly: they have *finite precision*.  Maybe this means that 0.3 just cannot be represented exactly.  If this is the case we can \"work around\" the problem by dividing by \"0.3\".  This should give us \"1\", exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.1 + 0.2) / 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it does not!  There is even more going on.\n",
    "\n",
    "As one example of how this can be a big problem in numerical work, suppose this calculation was the cosine of an angle.  In other words, suppose $\\cos\\theta = (0.1+0.2)/0.3$.  We know this means that $\\theta=0$ (or any integer multiple of $2\\pi$), but what happens when we try to calculate this numerically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40189/2883233312.py:1: RuntimeWarning: invalid value encountered in arccos\n",
      "  np.arccos((0.1 + 0.2) / 0.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arccos((0.1 + 0.2) / 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an error and the dreaded `nan`.  NaN means \"not a number\", it is pretty bad when you do a calculation with numbers and the result is not even a number!  Even though this is simple to calculate analytically, it produces an error when computed numerically. Though this is a contrived example, this sort of issue shows up far more frequently than we would like!  (Yes, I have encountered precisely this type of error more than once.)\n",
    "\n",
    "Another straight forward example of an error that can occur is using a square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40189/1073257806.py:1: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(0.3 - (0.1 + 0.2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.3 - (0.1 + 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Precision\n",
    "\n",
    "At the heart of the issue is the fact that only a finite number of digits (finite amount of information) can be stored.  In fact we have encountered this issue before.  Consider\n",
    "$$ \\frac{1}{3} = 0.33333333333333333333\\ldots .$$\n",
    "There are infinitely many digits in this expression.  We cannot write out an infinite number of digits so let us truncate it to three decimal places\n",
    "$$ \\frac{1}{3} = 0.333 . $$\n",
    "Suppose we calculate the sum of three such terms, we find\n",
    "$$ \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 0.999 . $$\n",
    "Notice this is **not equal to one**!\n",
    "\n",
    "If we add in more digits that does not help.  No matter how many digits we include we still do not get one!\n",
    "\n",
    "This is an example that shows up since we are using base 10 to represent the fraction $1/3$.  It cannot be exactly represented in base 10 by a finite number of digits.  Many other numbers can be, for example, $\\frac{1}{10} = 0.1$ is exactly represented, at least in base 10.\n",
    "\n",
    "## Floating Point Representation\n",
    "\n",
    "We have seen that all real numbers cannot be exactly represented in base 10.  The same is true in every other base.  Computer hardware (almost always) uses base 2 to represent numbers.  Again, all real numbers cannot be exactly represented in base 2 either (or any finite base, for that matter).  Furthermore, computers have finite storage space so can only store a finite number of digits for any number.  They must truncate numbers, much like our $1/3$ example above.\n",
    "\n",
    "*This fact is the single most important and confusing thing about numerical work.*  Countless hours are wasted tracking down problems caused by this issue!\n",
    "\n",
    "We will not go into the exact details of how numbers are stored, how calculations and round off is handled, *etc.*, as there are many technical details.  We will, however, try to get a basic idea.\n",
    "\n",
    "### IEEE 754 Standard\n",
    "\n",
    "How to represent numbers, and more importantly, how to handle calculations with truncated numbers and round off, has been standardized in the *IEEE 754 standard*.  A somewhat readable discussion is available from [Wikipedia](https://en.wikipedia.org/wiki/IEEE_754).  The standard reference everyone is sent to when they ask this question in a public forum is [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html).  A greatly simplified discussion can be found at [\n",
    "What Every Programmer Should Know About Floating-Point Arithmetic](http://floating-point-gui.de/).  This is just a small sample of the material available online.  It is a common and confusing question that comes up when we first start using computers to solve numerical problems.\n",
    "\n",
    "We can see how a number is actually stored using the following code.  It comes from (with some tweaks to make it work for us) a [Stack Overflow post](https://stackoverflow.com/questions/21895756/why-are-floating-point-numbers-inaccurate).  See that post for a more complete discussion.  You do not need to understand this code, it is just here for us to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import itertools\n",
    "def float_to_bin_parts(number, bits=64):\n",
    "    if bits == 64:      # double precision. Default for a Python and NumPy floats.\n",
    "        int_pack      = 'Q'\n",
    "        float_pack    = 'd'\n",
    "        exponent_bits = 11\n",
    "        mantissa_bits = 52\n",
    "        exponent_bias = 1023\n",
    "    elif bits == 32:    # single precision\n",
    "        int_pack      = 'I'\n",
    "        float_pack    = 'f'\n",
    "        exponent_bits = 8\n",
    "        mantissa_bits = 23\n",
    "        exponent_bias = 127\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('bits argument must be 32 or 64')\n",
    "    bin_iter = iter(bin(struct.unpack(int_pack, struct.pack(float_pack, number))[0])[2:].rjust(bits, '0'))\n",
    "    return [''.join(itertools.islice(bin_iter, x)) for x in (1, exponent_bits, mantissa_bits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the number 0.1 as it is represented (in 64 bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '01111111011', '1001100110011001100110011001100110011001100110011010']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb = float_to_bin_parts(0.1)\n",
    "fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Interpreting the Data\n",
    "\n",
    "You will see that the return value has been split into three components. These components are:\n",
    "\n",
    "    Sign\n",
    "    Exponent\n",
    "    Mantissa (also called Significand or Fraction)\n",
    "    \n",
    "\n",
    "The short version of how to convert these values in base 2 when numbers are stored on a 64 bit computer (which is what pretty much everyone uses these days) is \n",
    "* Sign (1 bit): 0 for positive, 1 for negative\n",
    "* Exponent (11 bits): Subtract `2**[(# of bits) - 1] - 1` to get the true exponent\n",
    "* Mantissa (52 bits): Divide by `2**(# of bits)` and add `1` to get the true mantissa\n",
    "\n",
    "For our case the sign is `0` since this is a positive number.\n",
    "\n",
    "We can calculate the exponent as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = int(fb[1], 2) - (2**10 - 1)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the mantissa as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.60000000000000009'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = int(fb[2], 2) / 2**52 + 1\n",
    "format(m, '.17f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the this is not an exact decimal number.\n",
    "\n",
    "What all of this shows is that we can represent our decimal number, 0.1, as\n",
    "$$ 0.1 \\approx +1.6 \\times 2^{-4}. $$\n",
    "\n",
    "We can convert to decimal by calculating $m \\times 2^e$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10000000000000001'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(m * 2**(e), '.17f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is not exact in binary even though it is (of course) exact in decimal.\n",
    "\n",
    "We could do the same for 0.2 and 0.3 to see how the errors enter into our calculation.\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "For the representation of a number in the way given above (which corresponds to double precision) we get about 16 digits of accuracy.\n",
    "\n",
    "This does not mean that the smallest number we can have is $10^{-16}$.  It also does not mean that a number like $10^{-19}$ is (necessarily) less accurate than a number like $0.1$.  The important facts to keep in mind are :\n",
    "\n",
    "1. the number of digits corresponds to the number of bits in the mantissa, and\n",
    "2. the complete range of values we can have is determined by the number of bits in the exponent.\n",
    "\n",
    "There are 53 bits in the mantissa. (Notice that we said 52 above, however, we always choose to write our number such that the first binary digit is '1'. Thus, we do not need to including this digit and we get one extra bit of precision for \"free\".)  This means the number of digits in base 10 is\n",
    "$$ \\log_{10}(2^{53}) = 15.95\\ldots \\approx 16. $$\n",
    "\n",
    "There are 11 bits in the exponent, but one of them is to determine the sign, thus we really only have 10 bits.  This means that in base 10 the maximum exponent is\n",
    "$$ 2^{10} / \\log_2(10) \\approx 308. $$\n",
    "\n",
    "### Actual floating point information\n",
    "\n",
    "While this is nice, we can learn this and much more from `numpy` itself, in particular we can get all the floating point information we want from `np.finfo()`.  Some of this information is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = np.finfo(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits=64\n",
      "Max exp=1024, Max val=1.7976931348623157e+308\n",
      "Min exp=-1022, Min val=-1.7976931348623157e+308\n",
      "Smallest number = 2.2250738585072014e-308\n",
      "Smallest difference = 2.220446049250313e-16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Bits={fi.bits}\n",
    "Max exp={fi.maxexp}, Max val={fi.max}\n",
    "Min exp={fi.minexp}, Min val={fi.min}\n",
    "Smallest number = {fi.tiny}\n",
    "Smallest difference = {fi.eps}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the smallest number and the smallest difference are very different.  The smallest difference is the smallest number we can add and subtract one from such that it will be preserved, in other words,\n",
    "$$ (1+\\epsilon)-1 = \\epsilon $$\n",
    "is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=2.220446049250313e-16, eps=2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "x = 1 + fi.eps\n",
    "y = x - 1\n",
    "print(f\"y={y}, eps={fi.eps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose a smaller number this simple relation is no longer true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000000000000000'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1 + 1e-18\n",
    "y = x - 1\n",
    "format(y, \".17f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accurate step sizes\n",
    "\n",
    "As one example of how these issues show up and are worked around we consider the case of step sizes.  Many algorithms we want to implement require us to use small step sizes.  Since simple floating point numbers in base 10 are typically not exactly represented in base 2 it is not accurate to set a step size in base 10 and use it on a computer (which is in base 2).\n",
    "\n",
    "To understand what this means consider calculating a numerical derivative.  An algorithm we will see in the future called center differencing is given by\n",
    "$$ \\frac{\\mathrm{d}f(x)}{\\mathrm{d}x} = \\frac{f(x+h) - f(x-h)}{2h}, $$\n",
    "for some small value of $h$.\n",
    "\n",
    "Let us calculate the derivative of $f(x)=\\cos(x)$ for $x_0=\\pi/4$.  Analytically we know that $f'(x) = -\\sin(x)$.\n",
    "\n",
    "To accurately represent the step size we use the idiom\n",
    "$$ h = (x_0 + h_{10}) - x_0 $$\n",
    "where $h_{10}$ is the step size we specify in base 10.  As we have seen, due to finite precision we expect that $h\\ne h_{10}$.  As a specific example consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h10=1e-05, h=9.99999999995449e-06\n"
     ]
    }
   ],
   "source": [
    "h10 = 1e-5 # NOTE!!!\n",
    "x0 = np.pi / 4\n",
    "h = x0 + h10\n",
    "h = h - x0\n",
    "print(f\"h10={h10}, h={h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply these two to step sizes to the center differencing algorithm.  We see that it does make a difference!  The accurately represented step size gives a more accurate numerical derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h10 : fractional error = 1.974753693900766e-11\n",
      "h   : fractional error = 1.5196288671859293e-11\n"
     ]
    }
   ],
   "source": [
    "def center_difference(f, x, h):\n",
    "    return (f(x+h) - f(x-h)) / (2 * h)\n",
    "\n",
    "fp_h10 = center_difference(np.cos, x0, h10)\n",
    "fp_h = center_difference(np.cos, x0, h)\n",
    "fp_true = -np.sin(x0)\n",
    "print(f\"h10 : fractional error = {np.abs(1 - fp_h10 / fp_true)}\")\n",
    "print(f\"h   : fractional error = {np.abs(1 - fp_h / fp_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that using $h$ instead of $h_{10}$ gives a more accurate result!  Of course it is a very small difference here, but it \"cost us nothing\" to use an accurate step size and this type of error can add up quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
